{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KerasTest.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ojs8139/KISTI_Team1/blob/test1/KerasTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "TINVBBjlOCPb",
        "colab_type": "code",
        "outputId": "a9dc3b5b-518b-4ab7-99d3-1967c2c12252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "cell_type": "code",
      "source": [
        "# 0. 사용할 패키지 불러오기\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "# 1. 데이터셋 생성하기\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255.0\n",
        "x_test = x_test.reshape(10000, 784).astype('float32') / 255.0\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 2. 모델 구성하기\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, input_dim=28*28, activation='relu'))\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# 3. 모델 학습과정 설정하기\n",
        "model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
        "\n",
        "# 4. 모델 학습시키기\n",
        "hist = model.fit(x_train, y_train, epochs=5, batch_size=32)\n",
        "\n",
        "# 5. 학습과정 살펴보기\n",
        "print('## training loss and acc ##')\n",
        "print(hist.history['loss'])\n",
        "print(hist.history['acc'])\n",
        "\n",
        "# 6. 모델 평가하기\n",
        "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
        "print('## evaluation loss and_metrics ##')\n",
        "print(loss_and_metrics)\n",
        "\n",
        "# 7. 모델 사용하기\n",
        "xhat = x_test[0:1]\n",
        "yhat = model.predict(xhat)\n",
        "print('## yhat ##')\n",
        "print(yhat)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.6855 - acc: 0.8228\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.3484 - acc: 0.9023\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2992 - acc: 0.9154\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2705 - acc: 0.9232\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2480 - acc: 0.9297\n",
            "## training loss and acc ##\n",
            "[0.6855104200363159, 0.3483804863413175, 0.2992180430312951, 0.2704575920244058, 0.2479913293659687]\n",
            "[0.8228166666666666, 0.9022666666666667, 0.9154, 0.9232166666666667, 0.9297]\n",
            "10000/10000 [==============================] - 0s 31us/step\n",
            "## evaluation loss and_metrics ##\n",
            "[0.2308235692963004, 0.9344]\n",
            "## yhat ##\n",
            "[[2.0666017e-05 2.1390480e-07 4.4747500e-04 2.5686759e-03 1.2945167e-06\n",
            "  2.8575976e-05 8.2282561e-08 9.9632543e-01 1.8789187e-05 5.8883266e-04]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}